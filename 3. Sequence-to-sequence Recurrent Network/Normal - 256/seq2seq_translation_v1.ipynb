{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch import optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from io import open\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from os import system\n",
    "from argparse import ArgumentParser\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25000 - Loss: 2.3732 - Score: 0.0254 Current Max Score:  0.0254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dfa2a58c2f06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[0mencoder1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInput_Words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msos_eos_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHidden_Size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[0mattention_decoder1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttentionDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHidden_Size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutput_Words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msos_eos_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m \u001b[0mtrainIters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_decoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_per_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPrint_Per_Time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_per_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPlot_Per_Time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Max BLEU-4 Score: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTotal_Score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-dfa2a58c2f06>\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(encoder, decoder, n_iters, print_per_time, plot_per_time, learning_rate)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mprint_bleu_score_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcompute_test_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[0mplot_bleu_score_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcompute_test_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mTotal_Score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprint_bleu_score_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-dfa2a58c2f06>\u001b[0m in \u001b[0;36mcompute_test_score\u001b[1;34m()\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTest_Data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTest_Data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_decoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mbleu_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-dfa2a58c2f06>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_attention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m             \u001b[0mdecoder_attentions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_attention\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mtopv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-dfa2a58c2f06>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         attention_weights = F.softmax(\n\u001b[1;32m--> 111\u001b[1;33m             self.attention(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n\u001b[0m\u001b[0;32m    112\u001b[0m         attention_applied = torch.bmm(attention_weights.unsqueeze(0),\n\u001b[0;32m    113\u001b[0m                                  encoder_outputs.unsqueeze(0))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-----setting parameters-----\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "Teacher_Forcing_Ratio = 0.5\n",
    "\n",
    "Epochs = 25000\n",
    "Print_Per_Time = 10\n",
    "Plot_Per_Time = 1\n",
    "Hidden_Size = 512\n",
    "Dropout_P = 0.1 # Dropout probability --> to decrease the overfitting probability\n",
    "Learning_Rate = 0.01\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "Total_Score = []\n",
    "\n",
    "\n",
    "#-----Train dataset-----\n",
    "\n",
    "# Read datasets\n",
    "Train_Data = pd.read_json('train.json')\n",
    "Test_Data = pd.read_json('test.json')\n",
    "New_Test_Data = pd.read_json('new_test.json')\n",
    "Data_Pairs = [] # (inputs, targets)\n",
    "\n",
    "# Read data into \"Data_Pairs\" \n",
    "for data, target in zip(Train_Data['input'], Train_Data['target']):\n",
    "    # Let every data would be read\n",
    "    num = len(data)\n",
    "    for i in range(num):Data_Pairs.append([data[i], target])\n",
    "\n",
    "#-----Data Processing-----\n",
    "\n",
    "# Transfer the type of word\n",
    "class Words:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.sos_eos_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def Letter(self, letter):\n",
    "        if letter not in self.word2index:\n",
    "            self.word2index[letter] = self.sos_eos_words\n",
    "            self.word2count[letter] = 1\n",
    "            self.index2word[self.sos_eos_words] = letter\n",
    "            self.sos_eos_words += 1\n",
    "        else:\n",
    "            self.word2count[letter] += 1\n",
    "            \n",
    "    def Word(self, words):\n",
    "        for word in words:\n",
    "            for letter in word:\n",
    "                self.Letter(letter)\n",
    "\n",
    "# Distinguish the \"input\" and \"target\" in the dataset\n",
    "Input_Words = Words('input')\n",
    "Output_Words = Words('target')\n",
    "\n",
    "# Read a word to train per time\n",
    "for pair in Data_Pairs:\n",
    "    Input_Words.Word(pair[0])\n",
    "    Output_Words.Word(pair[1])\n",
    "\n",
    "#-----Encoder-----\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size) #LSTM\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden) #LSTM\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "#-----Decoder-----\n",
    "\n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size, dropout_p=Dropout_P, max_length=MAX_LENGTH):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attention_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attention_weights = F.softmax(\n",
    "            self.attention(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attention_applied[0]), 1)\n",
    "        output = self.attention_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attention_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    letters = list(sentence)\n",
    "    return [lang.word2index[letter] for letter in letters]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(Input_Words, pair[0])\n",
    "    target_tensor = tensorFromSentence(Output_Words, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < Teacher_Forcing_Ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_per_time = Print_Per_Time, plot_per_time = Plot_Per_Time, learning_rate=Learning_Rate):\n",
    "    plot_losses = []\n",
    "    plot_bleu_scores = []\n",
    "    print_loss_total = 0\n",
    "    print_bleu_score_total = 0\n",
    "    plot_loss_total = 0\n",
    "    plot_bleu_score_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(Data_Pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        print_bleu_score_total += compute_test_score()\n",
    "        plot_bleu_score_total += compute_test_score()\n",
    "        Total_Score.append(print_bleu_score_total)\n",
    "\n",
    "        if iter % print_per_time == 0:\n",
    "            print_loss_avg = print_loss_total / print_per_time\n",
    "            print_loss_total = 0\n",
    "            print_bleu_score_avg = print_bleu_score_total / print_per_time\n",
    "            print_bleu_score_total = 0\n",
    "            print('%d/%d - Loss: %.4f - Score: %.4f' % (iter, n_iters, print_loss_avg, print_bleu_score_avg), \"Current Max Score: \", round(max(Total_Score)/Print_Per_Time,4))\n",
    "\n",
    "        if iter % plot_per_time == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_per_time\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            plot_bleu_score_avg = plot_bleu_score_total / plot_per_time\n",
    "            plot_bleu_scores.append(plot_bleu_score_avg)\n",
    "            plot_bleu_score_total = 0\n",
    "    Plot('loss', plot_losses)\n",
    "    Plot('score', plot_bleu_scores)\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(Input_Words, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(Output_Words.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def compute_test_score():\n",
    "    bleu_score = []\n",
    "    for idx in Test_Data.index:\n",
    "        input = Test_Data.loc[idx, 'input'][0]\n",
    "        target = Test_Data.loc[idx, 'target']\n",
    "        output, attentions = evaluate(encoder1, attention_decoder1, input)\n",
    "        pred = \"\".join(output)\n",
    "        bleu_score.append(compute_bleu(target, pred))\n",
    "    return sum(bleu_score)/len(bleu_score)\n",
    "    \n",
    "#compute BLEU-4 score\n",
    "def compute_bleu(output, reference):\n",
    "    cc = SmoothingFunction()\n",
    "    if len(reference) == 3:\n",
    "        weights = (0.33,0.33,0.33)\n",
    "    else:\n",
    "        weights = (0.25,0.25,0.25,0.25)\n",
    "    return sentence_bleu([reference], output,weights=weights,smoothing_function=cc.method1)\n",
    "    \n",
    "def Plot(mode, data):\n",
    "    plt.plot(data)\n",
    "    plt.xlabel('Epochs')\n",
    "    if mode == 'loss':\n",
    "        plt.title('Training Loss')\n",
    "        plt.ylabel('CrossEntropy Loss')\n",
    "    elif mode == 'score':\n",
    "        plt.title('Bleu-4 Test Score')\n",
    "        plt.ylabel('Score')\n",
    "\n",
    "    name = mode+\"-512.png\"\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "    \n",
    "encoder1 = EncoderRNN(Input_Words.sos_eos_words, Hidden_Size).to(device)\n",
    "attention_decoder1 = AttentionDecoderRNN(Hidden_Size, Output_Words.sos_eos_words, dropout_p=0.1).to(device)\n",
    "trainIters(encoder1, attention_decoder1, Epochs, print_per_time = Print_Per_Time, plot_per_time = Plot_Per_Time)\n",
    "\n",
    "print(\"Max BLEU-4 Score: \", round(max(Total_Score), 4))\n",
    "print(\"----------\")\n",
    "\n",
    "Bleu_Score_Test = []\n",
    "\n",
    "for idx in Test_Data.index:\n",
    "    input = Test_Data.loc[idx, 'input'][0]\n",
    "    target = Test_Data.loc[idx, 'target']\n",
    "    \n",
    "    output, attentions = evaluate(encoder1, attention_decoder1, input)\n",
    "    pred = \"\".join(output)\n",
    "    print('='*30)\n",
    "    print('input:\\t', input)\n",
    "    print('target:\\t', target)\n",
    "    print('pred:\\t', pred)\n",
    "    \n",
    "    Bleu_Score_Test.append(compute_bleu(target, pred))\n",
    "\n",
    "print('='*30)\n",
    "print(\"Test Data BLEU-4 Score: \", round(sum(Bleu_Score_Test)/len(Bleu_Score_Test),4))\n",
    "print(\"----------\")\n",
    "\n",
    "Bleu_Score_New_Test = []\n",
    "\n",
    "for idx in New_Test_Data.index:\n",
    "    input = New_Test_Data.loc[idx, 'input'][0]\n",
    "    target = New_Test_Data.loc[idx, 'target']\n",
    "    \n",
    "    output, attentions = evaluate(encoder1, attention_decoder1, input)\n",
    "    pred = \"\".join(output)\n",
    "    print('='*30)\n",
    "    print('input:\\t', input)\n",
    "    print('target:\\t', target)\n",
    "    print('prediction:\\t', pred)\n",
    "    \n",
    "    Bleu_Score_New_Test.append(compute_bleu(target, pred))\n",
    "\n",
    "print('='*30)\n",
    "print(\"New Test Data BLEU-4 Score: \", round(sum(Bleu_Score_New_Test)/len(Bleu_Score_New_Test),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
