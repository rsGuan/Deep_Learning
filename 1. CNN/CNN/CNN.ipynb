{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "309832006_Lab1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAfdrNspYErD"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#####################################################################################################################\n",
        "def sigmoid(x):\n",
        "    \"\"\" Sigmoid function.\n",
        "    This function accepts any shape of np.ndarray object as input and perform sigmoid operation.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def der_sigmoid(y):\n",
        "    \"\"\" First derivative of Sigmoid function.\n",
        "    The input to this function should be the value that output from sigmoid function.\n",
        "    \"\"\"\n",
        "    return sigmoid(y) * (1 - sigmoid(y))\n",
        "#####################################################################################################################\n",
        "class GenData:\n",
        "    @staticmethod\n",
        "    def _gen_linear(n=100):\n",
        "        \"\"\" Data generation (Linear)\n",
        "\n",
        "        Args:\n",
        "            n (int):    the number of data points generated in total.\n",
        "\n",
        "        Returns:\n",
        "            data (np.ndarray, np.float):    the generated data with shape (n, 2). Each row represents\n",
        "                a data point in 2d space.\n",
        "            labels (np.ndarray, np.int):    the labels that correspond to the data with shape (n, 1).\n",
        "                Each row represents a corresponding label (0 or 1).\n",
        "        \"\"\"\n",
        "        data = np.random.uniform(0, 1, (n, 2))\n",
        "\n",
        "        inputs = []\n",
        "        labels = []\n",
        "\n",
        "        for point in data:\n",
        "            inputs.append([point[0], point[1]])\n",
        "\n",
        "            if point[0] > point[1]:\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "        return np.array(inputs), np.array(labels).reshape((-1, 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _gen_xor(n=100):\n",
        "        \"\"\" Data generation (XOR)\n",
        "\n",
        "        Args:\n",
        "            n (int):    the number of data points generated in total.\n",
        "\n",
        "        Returns:\n",
        "            data (np.ndarray, np.float):    the generated data with shape (n, 2). Each row represents\n",
        "                a data point in 2d space.\n",
        "            labels (np.ndarray, np.int):    the labels that correspond to the data with shape (n, 1).\n",
        "                Each row represents a corresponding label (0 or 1).\n",
        "        \"\"\"\n",
        "        data_x = np.linspace(0, 1, n // 2)\n",
        "\n",
        "        inputs = []\n",
        "        labels = []\n",
        "\n",
        "        for x in data_x:\n",
        "            inputs.append([x, x])\n",
        "            labels.append(0)\n",
        "\n",
        "            if x == 1 - x:\n",
        "                continue\n",
        "\n",
        "            inputs.append([x, 1 - x])\n",
        "            labels.append(1)\n",
        "\n",
        "        return np.array(inputs), np.array(labels).reshape((-1, 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def fetch_data(mode, n):\n",
        "        \"\"\" Data gather interface\n",
        "\n",
        "        Args:\n",
        "            mode (str): 'Linear' or 'XOR', indicate which generator is used.\n",
        "            n (int):    the number of data points generated in total.\n",
        "        \"\"\"\n",
        "        assert mode == 'Linear' or mode == 'XOR'\n",
        "\n",
        "        data_gen_func = {\n",
        "            'Linear': GenData._gen_linear,\n",
        "            'XOR': GenData._gen_xor\n",
        "        }[mode]\n",
        "\n",
        "        return data_gen_func(n)\n",
        "#####################################################################################################################\n",
        "class SimpleNet:\n",
        "    def __init__(self, num_step=2000, print_interval=100):\n",
        "        \"\"\" A hand-crafted implementation of simple network.\n",
        "\n",
        "        Args:\n",
        "            num_step (optional):    the total number of training steps.\n",
        "            print_interval (optional):  the number of steps between each reported number.\n",
        "        \"\"\"\n",
        "        self.num_step = num_step\n",
        "        self.print_interval = print_interval\n",
        "\n",
        "        # Model parameters initialization\n",
        "        # hidden layer 1: 100 nodes\n",
        "        # hidden layer 2: 10 nodes\n",
        "        # Please initiate your network parameters here.\n",
        "        \n",
        "        # 輸入node數\n",
        "        InputNodes = 2\n",
        "        # hidden layer1的node數\n",
        "        HiddenLayerNodes1 = 100\n",
        "        # hidden layer1的node數\n",
        "        HiddenLayerNodes2 = 10\n",
        "        # 輸出node數\n",
        "        OutputNodes = 1\n",
        "        \n",
        "        self.hidden1_weights = np.random.normal(size = (InputNodes, HiddenLayerNodes1)) #w1 #2*100\n",
        "        self.hidden2_weights = np.random.normal(size = (HiddenLayerNodes1, HiddenLayerNodes2)) #w2 #100*10\n",
        "        self.output_weights = np.random.normal(size = (HiddenLayerNodes2, OutputNodes)) #w3 #10*1     \n",
        "\n",
        "    @staticmethod\n",
        "    def plot_result(self, data, gt_y, pred_y):\n",
        "        \"\"\" Data visualization with ground truth and predicted data comparison. There are two plots\n",
        "        for them and each of them use different colors to differentiate the data with different labels.\n",
        "\n",
        "        Args:\n",
        "            data:   the input data\n",
        "            gt_y:   ground truth to the data\n",
        "            pred_y: predicted results to the data\n",
        "        \"\"\"\n",
        "        assert data.shape[0] == gt_y.shape[0]\n",
        "        assert data.shape[0] == pred_y.shape[0]\n",
        "\n",
        "        plt.figure()\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title('Ground Truth', fontsize=18)\n",
        "\n",
        "        for idx in range(data.shape[0]):\n",
        "            if gt_y[idx] == 0:\n",
        "                plt.plot(data[idx][0], data[idx][1], 'ro')\n",
        "            else:\n",
        "                plt.plot(data[idx][0], data[idx][1], 'bo')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title('Prediction', fontsize=18)\n",
        "\n",
        "        for idx in range(data.shape[0]):\n",
        "            if pred_y[idx] == 0:\n",
        "                plt.plot(data[idx][0], data[idx][1], 'ro')\n",
        "            else:\n",
        "                plt.plot(data[idx][0], data[idx][1], 'bo')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        \n",
        "        plt.figure()\n",
        "\n",
        "        plt.title('Training Loss Plots', fontsize=18)\n",
        "\n",
        "        epoch = self.epoch\n",
        "        plt.plot(epoch, self.loss)\n",
        "        \n",
        "        plt.show()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Implementation of the forward pass.\n",
        "        It should accepts the inputs and passing them through the network and return results.\n",
        "        \"\"\"\n",
        "\n",
        "        #hidden layer 1 - X to Z1\n",
        "        self.Z1 = np.dot(inputs, self.hidden1_weights) #A1 #1*100\n",
        "        self.Z1_sig = sigmoid(self.Z1) #Z1 \n",
        "\n",
        "        #hidden layer 2 - Z1 to Z2\n",
        "        self.Z2 = np.dot(self.Z1_sig, self.hidden2_weights) #A2 #1*10\n",
        "        self.Z2_sig = sigmoid(self.Z2) #Z2\n",
        "        \n",
        "        #output - Z2 to Y\n",
        "        self.Y = np.dot(self.Z2_sig, self.output_weights) #A3 #1*1\n",
        "        self.Y_sig = sigmoid(self.Y) #Z3\n",
        "\n",
        "        return self.Y_sig\n",
        "\n",
        "    def backward(self, inputs):\n",
        "        \"\"\" Implementation of the backward pass.\n",
        "        It should utilize the saved loss to compute gradients and update the network all the way to the front.\n",
        "        \"\"\"\n",
        "        learning_rate = 1e-2\n",
        "\n",
        "        self.Z3_derSig = der_sigmoid(self.Y) #1*1\n",
        "        self.Z2_derSig = der_sigmoid(self.Z2) #1*10\n",
        "        self.Z1_derSig = der_sigmoid(self.Z1) #1*100\n",
        "        \n",
        "        #Y to Z2\n",
        "        self.YtoZ2_1 = np.dot(self.error, self.Z3_derSig.T) #1*der_sig(A3) #(1*1)(1*1)=(1*1)\n",
        "        self.YtoZ2 = np.dot(self.YtoZ2_1, self.Z2_sig) #1*der_sig(A3)*Z2 #(1*1)(1*10) = (1*10)\n",
        "        #Z2 to Z1\n",
        "        self.Z2toZ1_1 = np.dot(self.output_weights, self.YtoZ2_1) #1*der_sig(A3)*w3 #(1*1)(1*10)=(1*10) #數組與矩陣相乘，輸出矩陣大小        \n",
        "        self.Z2toZ1_2 = np.multiply(self.Z2toZ1_1, self.Z2_derSig.T) #1*der_sig(A3)*w3*der_sig(A2) #(10*1)(1*10)=(1*10)\n",
        "        self.Z2toZ1 = np.dot(self.Z2toZ1_2, self.Z1_sig) #1*der_sig(A3)*w3*der_sig(A2) * Z1 #(1*10)(1*100)=(1*100)\n",
        "        \n",
        "        #Z1 to X\n",
        "        self.Z1toX_1 = np.dot(self.hidden2_weights, self.Z2toZ1_2) #1*der_sig(A3)*w3*der_sig(A2) * w2 #(1*10)(10*100)= 1*100\n",
        "        self.Z1toX_2 = np.multiply(self.Z1toX_1, self.Z1_derSig.T) #1*der_sig(A3)*w3*der_sig(A2) * w2 * der_sig(A1) #(100*10)(1*100)=(1*100)\n",
        "        self.Z1toX = np.dot(self.Z1toX_2, inputs) #1*der_sig(A3)*w3*der_sig(A2) * w2 * der_sig(A1) * x #(10*100)(1*2)\n",
        "\n",
        "        self.hidden1_weights = self.hidden1_weights - learning_rate * self.Z1toX.T\n",
        "        self.hidden2_weights = self.hidden2_weights - learning_rate * self.Z2toZ1.T\n",
        "        self.output_weights = self.output_weights - learning_rate * self.YtoZ2.T\n",
        "\n",
        "    def train(self, inputs, labels):\n",
        "        \"\"\" The training routine that runs and update the model.\n",
        "\n",
        "        Args:\n",
        "            inputs: the training (and testing) data used in the model.\n",
        "            labels: the ground truth of correspond to input data.\n",
        "        \"\"\"\n",
        "        # make sure that the amount of data and label is match\n",
        "        assert inputs.shape[0] == labels.shape[0]\n",
        "\n",
        "        n = inputs.shape[0]\n",
        "        \n",
        "        self.loss = []\n",
        "        self.epoch = []\n",
        "        \n",
        "        for epochs in range(self.num_step):\n",
        "            for idx in range(n):\n",
        "                # operation in each training step:\n",
        "                #   1. forward passing\n",
        "                #   2. compute loss\n",
        "                #   3. propagate gradient backward to the front\n",
        "                self.output = self.forward(inputs[idx:idx+1, :])\n",
        "                self.error = self.output - labels[idx:idx+1, :]\n",
        "                self.backward(inputs[idx:idx+1, :])\n",
        "\n",
        "            if epochs % self.print_interval == 0:\n",
        "                \n",
        "                if(epochs % 3 == 0):\n",
        "                    print()\n",
        "                \n",
        "                print('Epochs {}: '.format(epochs), end=' ')\n",
        "                self.test(inputs, labels)\n",
        "                \n",
        "                self.epoch.append(epochs)\n",
        "                self.loss.append(abs(self.error[0]))\n",
        "        \n",
        "        print('Training finished')\n",
        "        self.test(inputs, labels)\n",
        "\n",
        "    def test(self, inputs, labels):\n",
        "        \"\"\" The testing routine that run forward pass and report the accuracy.\n",
        "\n",
        "        Args:\n",
        "            inputs: the testing data. One or several data samples are both okay.\n",
        "                The shape is expected to be [BatchSize, 2].\n",
        "            labels: the ground truth correspond to the inputs.\n",
        "        \"\"\"\n",
        "        n = inputs.shape[0]\n",
        "\n",
        "        error = 0.0\n",
        "        for idx in range(n):\n",
        "            result = self.forward(inputs[idx:idx+1, :])\n",
        "            error += abs(result - labels[idx:idx+1, :])\n",
        "\n",
        "        error /= n\n",
        "\n",
        "        \"\"\" Print or plot your results in your preferred forms\"\"\"\n",
        "\n",
        "        print('accuracy: %.2f' % ((1 - error)*100) + '%',end=', ')\n",
        "        print('loss: %.6f' % error,end='| ')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKAeANx-YZyx"
      },
      "source": [
        "# Run \"Linear\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0da8q0AkYhhu",
        "outputId": "40b5bb1b-7fce-4a36-f553-f4e7a67813a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    \"\"\" Customize your own code if needed \"\"\"\n",
        "\n",
        "    data, label = GenData.fetch_data('Linear', 100)\n",
        "    net = SimpleNet(500000, 1000)\n",
        "    net.train(data, label)\n",
        "\n",
        "    pred_result = np.round(net.forward(data))\n",
        "    SimpleNet.plot_result(net, data, label, pred_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epochs 0:  accuracy: 49.61%, loss: 0.503857| Epochs 1000:  accuracy: 93.94%, loss: 0.060556| Epochs 2000:  accuracy: 96.33%, loss: 0.036691| \n",
            "Epochs 3000:  accuracy: 97.32%, loss: 0.026831| Epochs 4000:  accuracy: 97.87%, loss: 0.021315| Epochs 5000:  accuracy: 98.22%, loss: 0.017752| \n",
            "Epochs 6000:  accuracy: 98.47%, loss: 0.015250| Epochs 7000:  accuracy: 98.66%, loss: 0.013394| Epochs 8000:  accuracy: 98.80%, loss: 0.011962| \n",
            "Epochs 9000:  accuracy: 98.92%, loss: 0.010825| Epochs 10000:  accuracy: 99.01%, loss: 0.009899| Epochs 11000:  accuracy: 99.09%, loss: 0.009131| \n",
            "Epochs 12000:  accuracy: 99.15%, loss: 0.008483| Epochs 13000:  accuracy: 99.21%, loss: 0.007930| Epochs 14000:  accuracy: 99.25%, loss: 0.007452| \n",
            "Epochs 15000:  accuracy: 99.30%, loss: 0.007035| Epochs 16000:  accuracy: 99.33%, loss: 0.006667| Epochs 17000:  accuracy: 99.37%, loss: 0.006341| \n",
            "Epochs 18000:  accuracy: 99.40%, loss: 0.006049| Epochs 19000:  accuracy: 99.42%, loss: 0.005786| Epochs 20000:  accuracy: 99.45%, loss: 0.005549| \n",
            "Epochs 21000:  accuracy: 99.47%, loss: 0.005333| Epochs 22000:  accuracy: 99.49%, loss: 0.005136| Epochs 23000:  accuracy: 99.50%, loss: 0.004955| \n",
            "Epochs 24000:  accuracy: 99.52%, loss: 0.004789| Epochs 25000:  accuracy: 99.54%, loss: 0.004635| Epochs 26000:  accuracy: 99.55%, loss: 0.004492| \n",
            "Epochs 27000:  accuracy: 99.56%, loss: 0.004360| Epochs 28000:  accuracy: 99.58%, loss: 0.004236| Epochs 29000:  accuracy: 99.59%, loss: 0.004121| \n",
            "Epochs 30000:  accuracy: 99.60%, loss: 0.004012| Epochs 31000:  accuracy: 99.61%, loss: 0.003911| Epochs 32000:  accuracy: 99.62%, loss: 0.003815| \n",
            "Epochs 33000:  accuracy: 99.63%, loss: 0.003725| Epochs 34000:  accuracy: 99.64%, loss: 0.003640| Epochs 35000:  accuracy: 99.64%, loss: 0.003559| \n",
            "Epochs 36000:  accuracy: 99.65%, loss: 0.003483| Epochs 37000:  accuracy: 99.66%, loss: 0.003410| Epochs 38000:  accuracy: 99.67%, loss: 0.003341| \n",
            "Epochs 39000:  accuracy: 99.67%, loss: 0.003276| "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uyyLm_oYrhN"
      },
      "source": [
        "# run \"XOR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn0gyVJmYlqA"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    \"\"\" Customize your own code if needed \"\"\"\n",
        "    \n",
        "    data, label = GenData.fetch_data('XOR', 100)\n",
        "\n",
        "    net = SimpleNet(1000000, 1000)\n",
        "    net.train(data, label)\n",
        "\n",
        "    pred_result = np.round(net.forward(data))\n",
        "    SimpleNet.plot_result(net, data, label, pred_result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}